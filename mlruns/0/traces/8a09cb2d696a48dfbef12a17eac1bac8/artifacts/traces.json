{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0xd21bc4f4f01b6698", "trace_id": "0x19c4fa8b8a2a3e876dbff7cb8d224ffa"}, "parent_id": null, "start_time": 1723665975188180100, "end_time": 1723665976211973100, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"8a09cb2d696a48dfbef12a17eac1bac8\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"How to run mlflow.evaluate()?\"}", "mlflow.spanOutputs": "{\"result\": \" I don't know.\", \"source_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 1}, \"page_content\": \"Page  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 0}, \"page_content\": \"OPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 19}, \"page_content\": \"Page  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 50}, \"page_content\": \"Page  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\", \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0x8e267a6ef361c7c4", "trace_id": "0x19c4fa8b8a2a3e876dbff7cb8d224ffa"}, "parent_id": "0xd21bc4f4f01b6698", "start_time": 1723665975209328100, "end_time": 1723665975531324700, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"8a09cb2d696a48dfbef12a17eac1bac8\"", "mlflow.spanType": "\"RETRIEVER\"", "metadata": "{\"ls_retriever_name\": \"vectorstore\", \"ls_vector_store_provider\": \"FAISS\", \"ls_embedding_provider\": \"OpenAIEmbeddings\"}", "mlflow.spanInputs": "\"How to run mlflow.evaluate()?\"", "mlflow.spanOutputs": "[{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 1}, \"page_content\": \"Page  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 0}, \"page_content\": \"OPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 19}, \"page_content\": \"Page  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 50}, \"page_content\": \"Page  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\", \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0xd4d17f6f5fc20f7e", "trace_id": "0x19c4fa8b8a2a3e876dbff7cb8d224ffa"}, "parent_id": "0xd21bc4f4f01b6698", "start_time": 1723665975532181600, "end_time": 1723665976211973100, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"8a09cb2d696a48dfbef12a17eac1bac8\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 1}, \"page_content\": \"Page  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 0}, \"page_content\": \"OPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 19}, \"page_content\": \"Page  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 50}, \"page_content\": \"Page  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\", \"type\": \"Document\"}], \"question\": \"How to run mlflow.evaluate()?\"}", "mlflow.spanOutputs": "{\"output_text\": \" I don't know.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x97c911268fdbeeb3", "trace_id": "0x19c4fa8b8a2a3e876dbff7cb8d224ffa"}, "parent_id": "0xd4d17f6f5fc20f7e", "start_time": 1723665975534192200, "end_time": 1723665976211973100, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"8a09cb2d696a48dfbef12a17eac1bac8\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"How to run mlflow.evaluate()?\", \"context\": \"Page  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\\n\\nOPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\\n\\nPage  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\\n\\nPage  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\"}", "mlflow.spanOutputs": "{\"text\": \" I don't know.\"}"}, "events": [{"name": "text", "timestamp": 1723665975534192, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nPage  1                       \n \n \nOPERATING  SYSTEM  \n \n \nLECTURE  NOTES\n\nOPERATING  SYSTEM   \n \n \n \nLecture Notes  \nOn \n \n \n \n \n \n \nPrepared by,  \n                                               A.SANDEEP.  \n                                                                                    \n                                                                                            ASSISTANT PROFESSOR . \n                                                                                                \n                                                                                              CSE\n\nPage  19                       \n\u2726 switching  to user mode  \n\u2726 jumping  to the proper  location  in the user program  to restart  that program  \nDispatch  latency  \u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \nScheduling  Criteria  \n\uf0b7 CPU  utilization  \u2013 keep  the CPU as busy as possible  \n\uf0b7 Throughput  \u2013 # of processes  that complete  their execution  per time unit \n\uf0b7 Turnaround  time \u2013 amount  of time to execute  a particular  process  \n\uf0b7 Waiting  time \u2013 amount  of time a process  has been  waiting in  the ready  queue  \n\uf0b7 Response  time \u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \nproduced,  not output  (for time-sharing  environment)  \nOptimization  Criteria  \n\uf0b7 Max CPU  utilization  \n\uf0b7 Max throughput  \n\uf0b7 Min turnaround  time \n\uf0b7 Min waiting  time \n\uf0b7 Min response  time \n \n \nFirst -Come,  First -Served  Scheduling  \nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \nin milliseconds:  \nProcess  Burst  Time  \nPI 24 \nP2 3 \nP3 3 \nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \n \nfollowing  Gantt  chart:  \nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \n \nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \ntimes  vary greatly.  \nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \nback  to the I/O queues.  At this point,  the CPU sits idle.\n\nPage  52                       \n \n \n \n \n \n \n \nstate.  b. Request i < Work.  \nIf no such  i exists,  go to step 4.  \n3. Work := Work  + Allocation i \nFinish[i]  := true \ngo to step 2. \n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \nif Finish[i]  =false,  then process Pi is deadlocked.  \nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \n \nRECOVERY  FROM DEADLOCK  \n \nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \n1. Process  Termination  \n2. Resource  preemption  \n3. \nProcess  Termination  \n \nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \ndeadlocked.  \n \nResource  Preemption  \n \nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \nthese  resources to  other processes until  the deadlock  cycle  is broken.  \nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \npreempted  based  on cost factor.  \n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \nstate.  \n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \nsame  process?  \n \n \nMEMORY  MANAGEMENT  \nIn a uni -programming system, main memory is divided into two parts: one part for the  \noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \nexecuted.  \nIn a multiprogramming system, the \u201cuser\u201d part of memory must be further subdivided  \nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \noperating  system and is  known as memory  management . \nBinding  of Instructions  and  Data  to Memory  \nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \nstages.  \n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\n\nQuestion: How to run mlflow.evaluate()?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0x78dfad730729fe14", "trace_id": "0x19c4fa8b8a2a3e876dbff7cb8d224ffa"}, "parent_id": "0x97c911268fdbeeb3", "start_time": 1723665975534192200, "end_time": 1723665976211192200, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"8a09cb2d696a48dfbef12a17eac1bac8\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nPage  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\\n\\nOPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\\n\\nPage  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\\n\\nPage  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\\n\\nQuestion: How to run mlflow.evaluate()?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" I don't know.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"total_tokens\": 1690, \"completion_tokens\": 5, \"prompt_tokens\": 1685}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"How to run mlflow.evaluate()?\"}", "response": "{\"result\": \" I don't know.\", \"source_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 1}, \"page_content\": \"Page  1                       \\n \\n \\nOPERATING  SYSTEM  \\n \\n \\nLECTURE  NOTES\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 0}, \"page_content\": \"OPERATING  SYSTEM   \\n \\n \\n \\nLecture Notes  \\nOn \\n \\n \\n \\n \\n \\n \\nPrepared by,  \\n                                               A.SANDEEP.  \\n                                                                                    \\n                                                                                            ASSISTANT PROFESSOR . \\n                                                                                                \\n                                                                                              CSE\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 19}, \"page_content\": \"Page  19                       \\n\\u2726 switching  to user mode  \\n\\u2726 jumping  to the proper  location  in the user program  to restart  that program  \\nDispatch  latency  \\u2013 time it takes  for the dispatcher  to stop one process  and start another  running.  \\nScheduling  Criteria  \\n\\uf0b7 CPU  utilization  \\u2013 keep  the CPU as busy as possible  \\n\\uf0b7 Throughput  \\u2013 # of processes  that complete  their execution  per time unit \\n\\uf0b7 Turnaround  time \\u2013 amount  of time to execute  a particular  process  \\n\\uf0b7 Waiting  time \\u2013 amount  of time a process  has been  waiting in  the ready  queue  \\n\\uf0b7 Response  time \\u2013 amount  of time it takes  from when  a request  was submitted  until the first response  is \\nproduced,  not output  (for time-sharing  environment)  \\nOptimization  Criteria  \\n\\uf0b7 Max CPU  utilization  \\n\\uf0b7 Max throughput  \\n\\uf0b7 Min turnaround  time \\n\\uf0b7 Min waiting  time \\n\\uf0b7 Min response  time \\n \\n \\nFirst -Come,  First -Served  Scheduling  \\nBy far the simplest CPU -scheduling algorithm is the first -come, first -served (FCFS) scheduling algorithm.  \\nWith this scheme, the process that requests the CPU first is allocated the CPU first. The implementation of the  \\nFCFS policy is easily managed with a FIFO queue. When a process enters the ready queue, its PCB is linked  \\nonto the tail of  the queue. When the CPU is free, it is allocated to the process at the head of the queue. The  \\nrunning  process  is then removed from  the queue.  The code for  FCFS scheduling  is simple to  write  and \\nunderstand.  The average waiting time  under  the FCFS policy, h owever,  is often quite long.  \\nConsider the following set of processes that arrive at time 0, with the length of the CPU -burst time given  \\nin milliseconds:  \\nProcess  Burst  Time  \\nPI 24 \\nP2 3 \\nP3 3 \\nIf the processes  arrive  in the order  PI, P2, P3, and are served  in FCFS  order,  we get the result  shown  in the \\n \\nfollowing  Gantt  chart:  \\nThe waiting  time is 0 milliseconds  for process  PI, 24 milliseconds  for process  PZ, and 27 milliseconds  for \\nprocess P3. Thus, the average waiting time is (0 + 24 + 27)/3 = 17 milliseconds. If the processes arrive in the  \\norder  P2, P3,  Pl, however,  the results  will be  as shown  in the following  Gantt  chart:  \\n \\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction is substantial. Thus, the average  \\nwaiting time under a FCF S policy is generally not minimal, and may vary substantially if the process CPU -burst  \\ntimes  vary greatly.  \\nIn addition, consider the performance of FCFS scheduling in a dynamic situation. Assume we have one  \\nCPU -bound process and many I/O -bound processes. As the processes flow around the system, the following  \\nscenario may result. The CPU -bound process will get the CPU and hold it. During this time, all the other  \\nprocesses  will finish  their I/O and move  into the ready  queue,  waiting  for the CPU.  While  the processes  wait in \\nthe ready queue, the I/O devices are idle. Eventually, the CPU -bound process finishes its CPU burst and moves  \\nto an I/O device.  All the I/O-bound  processes,  which  have  very short  CPU bursts,  execute  quickly  and move  \\nback  to the I/O queues.  At this point,  the CPU sits idle.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 50}, \"page_content\": \"Page  52                       \\n \\n \\n \\n \\n \\n \\n \\nstate.  b. Request i < Work.  \\nIf no such  i exists,  go to step 4.  \\n3. Work := Work  + Allocation i \\nFinish[i]  := true \\ngo to step 2. \\n4. If Finish[i] = false, for some i, 1 < i < n, then the system is in a deadlock state.  \\nif Finish[i]  =false,  then process Pi is deadlocked.  \\nThis algorithm  requires  an order  of m x n2 operations  to detect  whether  the system  is in a deadlocked  \\n \\nRECOVERY  FROM DEADLOCK  \\n \\nWhen  a detection  algorithm  determines  that a deadlock  exists,  then the system  or operator  is \\nresponsible  for handling  deadlock  problem.  There  are two options  for breaking  a deadlock.  \\n1. Process  Termination  \\n2. Resource  preemption  \\n3. \\nProcess  Termination  \\n \\nThere  are two method  to eliminate  deadlocks  by terminating  a process  as follows:  \\n1. Abort all deadlocked processes:  This method will break the deadlock cycle clearly by terminating all  \\nprocess.  This method  is cost effective.  And it removes  the partial  computations  completed  by the processes.  \\n2. Abort one process at a time until the deadlock cycle is eliminated:  This method terminates one process  \\nat a time,  and invokes  a deadlock -detection  algorithm  to determine  whether  any processes  are still \\ndeadlocked.  \\n \\nResource  Preemption  \\n \\nIn resource  preemption,  the operator  or system  preempts  some  resources  from processes  and give \\nthese  resources to  other processes until  the deadlock  cycle  is broken.  \\nIf preemption  is required  to deal with deadlocks,  then three  issues need  to be addressed:  \\n1. Selecting  a victim:  The system  or operator  selects  which  resources  and which  processes  are to be \\npreempted  based  on cost factor.  \\n2. Rollback : The system  or operator  must  roll back  the process  to some  safe state  and restart  it from that \\nstate.  \\n3. Starvation : The system  or operator  should ensure that resources  will not always  be preempted from  the \\nsame  process?  \\n \\n \\nMEMORY  MANAGEMENT  \\nIn a uni -programming system, main memory is divided into two parts: one part for the  \\noperating  system  (resident  monitor,  kernel)  and  one part  for the user  program  currently  being  \\nexecuted.  \\nIn a multiprogramming system, the \\u201cuser\\u201d part of memory must be further subdivided  \\nto accommodate  multiple  processes.  The  task  of subdivision  is carried  out dynamically  by the \\noperating  system and is  known as memory  management . \\nBinding  of Instructions  and  Data  to Memory  \\nAddress  binding  of instructions  and  data  to memory  addresses  can happen  at three  different  \\nstages.  \\n1. Compile  time : The  compile  time  is the time  taken  to compile  the program  or source  code.  \\nDuring  compilation,  if memory  location  known  a priori,  then  it generates  absolute  codes.\", \"type\": \"Document\"}]}"}