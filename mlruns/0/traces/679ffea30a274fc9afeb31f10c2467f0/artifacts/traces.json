{"spans": [{"name": "RetrievalQA", "context": {"span_id": "0x6e375b9734714059", "trace_id": "0x91b60a99754e02668ba4232701ad4600"}, "parent_id": null, "start_time": 1723665973197889700, "end_time": 1723665975097484700, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"679ffea30a274fc9afeb31f10c2467f0\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"query\": \"What is MLflow?\"}", "mlflow.spanOutputs": "{\"result\": \" I don't know.\", \"source_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 5}, \"page_content\": \"Page  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 24}, \"page_content\": \"Page  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 76}, \"page_content\": \"Page  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 62}, \"page_content\": \"Page  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\", \"type\": \"Document\"}]}"}, "events": []}, {"name": "VectorStoreRetriever", "context": {"span_id": "0xceacb72808a2a0e8", "trace_id": "0x91b60a99754e02668ba4232701ad4600"}, "parent_id": "0x6e375b9734714059", "start_time": 1723665973238821900, "end_time": 1723665973700547800, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"679ffea30a274fc9afeb31f10c2467f0\"", "mlflow.spanType": "\"RETRIEVER\"", "metadata": "{\"ls_retriever_name\": \"vectorstore\", \"ls_vector_store_provider\": \"FAISS\", \"ls_embedding_provider\": \"OpenAIEmbeddings\"}", "mlflow.spanInputs": "\"What is MLflow?\"", "mlflow.spanOutputs": "[{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 5}, \"page_content\": \"Page  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 24}, \"page_content\": \"Page  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 76}, \"page_content\": \"Page  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 62}, \"page_content\": \"Page  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\", \"type\": \"Document\"}]"}, "events": []}, {"name": "StuffDocumentsChain", "context": {"span_id": "0x79601020916efe38", "trace_id": "0x91b60a99754e02668ba4232701ad4600"}, "parent_id": "0x6e375b9734714059", "start_time": 1723665973701553500, "end_time": 1723665975097484700, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"679ffea30a274fc9afeb31f10c2467f0\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"input_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 5}, \"page_content\": \"Page  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 24}, \"page_content\": \"Page  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 76}, \"page_content\": \"Page  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 62}, \"page_content\": \"Page  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\", \"type\": \"Document\"}], \"question\": \"What is MLflow?\"}", "mlflow.spanOutputs": "{\"output_text\": \" I don't know.\"}"}, "events": []}, {"name": "LLMChain", "context": {"span_id": "0x049135dedf63b078", "trace_id": "0x91b60a99754e02668ba4232701ad4600"}, "parent_id": "0x79601020916efe38", "start_time": 1723665973708378100, "end_time": 1723665975097484700, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"679ffea30a274fc9afeb31f10c2467f0\"", "mlflow.spanType": "\"CHAIN\"", "mlflow.spanInputs": "{\"question\": \"What is MLflow?\", \"context\": \"Page  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\\n\\nPage  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\\n\\nPage  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\\n\\nPage  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\"}", "mlflow.spanOutputs": "{\"text\": \" I don't know.\"}"}, "events": [{"name": "text", "timestamp": 1723665973708378, "attributes": {"text": "Prompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nPage  5                       \nLecture  #3 \nEvolution  of OS: \n1. Mainframe  Systems  \nReduce setup time by batching similar jobs Automatic job sequencing \u2013 automatically transfers control from one  \njob to another.  First rudimentary  \noperating  system.  Resident  monitor  \n\uf0dc initial  control  in monitor  \n\uf0dc control  transfers  to job \n\uf0dc when  job completes  control  transfers  pack to  monitor  \n2. Batch  Processing  Operating  System:  \n\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \n\uf0dc   It allowed  only  one program  at a time.  \n\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \n3. Multiprogramming  Operating  System:  \n\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \n\uf0dc   The concept  of multiprogramming  is described  as follows:  \n\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \nof jobs  from  job pool  into  main  memory  and begins  to execute.  \n\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \na multiprogramming system, the operating system simply switches to another job and executes.  \nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \n\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \n\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \n3. Time -Sharing/multitasking  Operating  Systems  \nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \n\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \n\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \n\uf0dc   The users  can interact  with  each  job while  it is running.  \nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \n4. Multiprocessor  Operating  Systems  \nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \nfaster.  \nMultiprocessor  systems  have  three  main  advantages:  \n\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \nThe  speed -up ratio  with  N processors  is less  than  N. \n\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \n\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \nslow  it down.  \n \nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\n\nPage  24                       \nLecture  #13,#14  \n \nThread  \nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \nit can do more  than one task at a time.  \nMotivation  \nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \nimplemented  as a separate  process with  several  thread  of control.  \n \nSingle -threaded  and multithreaded  \n \n \nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \nIn certain situations a single application may be required to perform several similar tasks. For example, a  \nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \nthreaded  process,  it would be able to  service  only one client  at a time.  \nOne solution is to have the server run as a single process that accepts requests. When the server  \nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \npurpose. This approach would multithread the web -server process. The server would create a separate thread  \nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \ncreate  another thread  to service  the request.  \nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \nseparate thread.  This allows  the server  to service  several  concurrent requests.  \nBenefits  \nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \ninstance,  a multithreaded web  browser  could  still allow  user interaction  \nin one thread  while  an image  is being  loaded  in another  thread.\n\nPage  78                       \nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \ndestroyed,  resulting  in a simple  graph  structure.  \n \n \n \n \nSECONDARY  STORAGE  STRUCTURE  \nDISKS  STRUCTURE   \nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \ninformation  by recording  it magnetically  on the platters.  \nA read -write  head  \"flies\"  just above  each  surface  of every  platter.  The heads  are \nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \ndisk  drives  is measured  in gigabytes.\n\nPage  64                       \n \n \n(Example  of segmentation)  \n \n \nVIRTUAL  MEMORY  \nVirtual memory is a technique that allows the execution of processes that may not be  \ncompletely in memory. Only part of the program needs to be in memory for execution. It  \nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \nmemory allows processes to easily share files and address spaces, and it provides an efficient  \nmechanism  for process  creation.  \nVirtual memory is the separation of user logical memory from physical memory. This  \nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \nonly a smaller physical memory is available. Virtual memory makes the task of programming  \nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \nmemory  available.\n\nQuestion: What is MLflow?\nHelpful Answer:\u001b[0m"}}]}, {"name": "OpenAI", "context": {"span_id": "0xd86d239907ded4de", "trace_id": "0x91b60a99754e02668ba4232701ad4600"}, "parent_id": "0x049135dedf63b078", "start_time": 1723665973710379300, "end_time": 1723665975097484700, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"679ffea30a274fc9afeb31f10c2467f0\"", "mlflow.spanType": "\"LLM\"", "invocation_params": "{\"model_name\": \"gpt-3.5-turbo-instruct\", \"temperature\": 0.0, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"n\": 1, \"logit_bias\": {}, \"max_tokens\": 256, \"_type\": \"openai\", \"stop\": null}", "options": "{\"stop\": null}", "batch_size": "1", "mlflow.spanInputs": "[\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nPage  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\\n\\nPage  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\\n\\nPage  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\\n\\nPage  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\\n\\nQuestion: What is MLflow?\\nHelpful Answer:\"]", "mlflow.spanOutputs": "{\"generations\": [[{\"text\": \" I don't know.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}]], \"llm_output\": {\"token_usage\": {\"total_tokens\": 2332, \"completion_tokens\": 5, \"prompt_tokens\": 2327}, \"model_name\": \"gpt-3.5-turbo-instruct\"}, \"run\": null}"}, "events": []}], "request": "{\"query\": \"What is MLflow?\"}", "response": "{\"result\": \" I don't know.\", \"source_documents\": [{\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 5}, \"page_content\": \"Page  5                       \\nLecture  #3 \\nEvolution  of OS: \\n1. Mainframe  Systems  \\nReduce setup time by batching similar jobs Automatic job sequencing \\u2013 automatically transfers control from one  \\njob to another.  First rudimentary  \\noperating  system.  Resident  monitor  \\n\\uf0dc initial  control  in monitor  \\n\\uf0dc control  transfers  to job \\n\\uf0dc when  job completes  control  transfers  pack to  monitor  \\n2. Batch  Processing  Operating  System:  \\n\\uf0dc This  type of OS accepts more than one jobs and these jobs are batched/ grouped together according to their  \\nsimilar  requirements.  This  is done  by computer  operator.  Whenever  the computer  becomes  available,  the \\nbatched  jobs  are sent  for execution  and gradually  the output  is sent  back  to the user.  \\n\\uf0dc   It allowed  only  one program  at a time.  \\n\\uf0dc This  OS is responsible  for scheduling  the jobs  according  to priority  and the resource  required.  \\n3. Multiprogramming  Operating  System:  \\n\\uf0dc This  type of OS is used to execute more than one jobs simultaneously by a single processor. it increases CPU  \\nutilization  by organizing  jobs  so that  the CPU  always  has one job to execute.  \\n\\uf0dc   The concept  of multiprogramming  is described  as follows:  \\n\\uf0d8 All the jobs  that  enter  the system  are stored  in the job pool(  in disc).  The operating  system  loads  a set \\nof jobs  from  job pool  into  main  memory  and begins  to execute.  \\n\\uf0d8 During  execution,  the job may  have  to wait  for some  task,  such  as an I/O operation,  to complete.  In \\na multiprogramming system, the operating system simply switches to another job and executes.  \\nWhen  that  job needs  to wait,  the CPU  is switched  to another  job, and so on. \\n\\uf0d8 When  the first  job finishes  waiting  and it gets  the CPU  back.  \\n\\uf0d8 As long  as at least  one job needs  to execute,  the CPU  is never  idle.  \\nMultiprogramming  operating  systems  use the mechanism  of job scheduling  and CPU scheduling.  \\n3. Time -Sharing/multitasking  Operating  Systems  \\nTime  sharing  (or multitasking)  OS is a logical  extension  of multiprogramming.  It provides  extra  facilities  such  as: \\n\\uf0dc   Faster  switching  between  multiple  jobs  to make  processing  faster.  \\n\\uf0dc Allows  multiple  users  to share  computer  system  simultaneously.  \\n\\uf0dc   The users  can interact  with  each  job while  it is running.  \\nThese systems use a concept of virtual memory for effective utilization of memory space. Hence, in this OS, no  \\njobs  are discarded.  Each  one is executed  using  virtual  memory  concept.  It uses  CPU  scheduling,  memory  \\nmanagement,  disc  management  and security  management.  Examples:  CTSS,  MULTICS,  CAL,  UNIX  etc. \\n4. Multiprocessor  Operating  Systems  \\nMultiprocessor  operating  systems are also known  as parallel OS or  tightly coupled OS. Such  operating  \\nsystems  have  more  than  one processor  in close  communication  that  sharing  the computer  bus,  the clock  and \\nsometimes  memory  and peripheral  devices.  It executes  multiple  jobs  at same  time  and makes  the processing  \\nfaster.  \\nMultiprocessor  systems  have  three  main  advantages:  \\n\\uf0dc Increased  throughput:  By increasing  the number  of processors,  the system  performs  more  work  in less time.  \\nThe  speed -up ratio  with  N processors  is less  than  N. \\n\\uf0dc Economy  of scale:  Multiprocessor  systems  can save  more  money  than  multiple  single -processor  systems,  \\nbecause  they  can share  peripherals,  mass  storage,  and power  supplies.  \\n\\uf0dc Increased  reliability:  If one processor  fails  to done  its task,  then  each  of the remaining  processors  must  pick  \\nup a share  of the work of the  failed  processor.  The failure  of one processor  will not  halt the system,  only  \\nslow  it down.  \\n \\nThe ability  to continue  providing  service  proportional  to the level  of surviving  hardware  is called  graceful  \\ndegradation.  Systems  designed  for graceful  degradation  are called  fault  tolerant.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 24}, \"page_content\": \"Page  24                       \\nLecture  #13,#14  \\n \\nThread  \\nA thread, sometimes called a lightweight process (LWP), is a basic unit of CPU utilization; it  comprises a  \\nthread ID, a program counter, a register set, and a stack. It shares with other threads belonging to the same  \\nprocess its code section, data section, and other operating -system resources, such as open files and signals. A  \\ntraditional  (or heavy weight)  process  has a single  thread  of control.  If the process  has multiple  threads  of control,  \\nit can do more  than one task at a time.  \\nMotivation  \\nMany software packages that run on modern desktop PCs are multithreaded.An application typically is  \\nimplemented  as a separate  process with  several  thread  of control.  \\n \\nSingle -threaded  and multithreaded  \\n \\n \\nEx: A web browser  might  have  one thread  display  images  or text while  another  thread  retrieves  data \\nfrom the network. A word processor may have a thread for displaying graphics, another thread for reading  \\nkeystrokes  from the user,  and a third thread  for performing  spelling  and grammar  checking  in the background.  \\nIn certain situations a single application may be required to perform several similar tasks. For example, a  \\nweb server accepts client requests for web pages, images, sound, and so forth. A busy web server may have  \\nseveral (perhaps hundreds) of clients concurrently accessing it. If the web server ran as a traditional single - \\nthreaded  process,  it would be able to  service  only one client  at a time.  \\nOne solution is to have the server run as a single process that accepts requests. When the server  \\nreceives a request, it creates a separate process to service that request. In fact, this process -creation me thod \\nwas in common use before threads became popular. Process creation is very heavyweight, as was shown in the  \\nprevious chapter. If the new process will perform the same tasks as the existing process, why incur all that  \\noverhead?  It is generally more effi cient  for one process  that contains  multiple  threads  to serve  the same  \\npurpose. This approach would multithread the web -server process. The server would create a separate thread  \\nthat would listen for client requests; when a request was made, rather than cr eating another process, it would  \\ncreate  another thread  to service  the request.  \\nThreads  also play a vital  role in remote procedure  call (RPC)  systems.  RPCs  allow inter -process  \\ncommunication  by providing  a communication  mechanism  similar  to ordinary  function  or procedure  calls.  \\nTypically, RPC servers are multithreaded. When a server receives a message, it services the message using a  \\nseparate thread.  This allows  the server  to service  several  concurrent requests.  \\nBenefits  \\nThe benefits  of multithreaded  programming  can be broken  down  into four major  categories:  \\n1. Responsiveness: Multithreading an interactive application may allow a program to continue running even if  \\npart of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. For  \\ninstance,  a multithreaded web  browser  could  still allow  user interaction  \\nin one thread  while  an image  is being  loaded  in another  thread.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 76}, \"page_content\": \"Page  78                       \\nWhen  we add links  to an existing  tree -structured  directory,  the tree  structure  is \\ndestroyed,  resulting  in a simple  graph  structure.  \\n \\n \\n \\n \\nSECONDARY  STORAGE  STRUCTURE  \\nDISKS  STRUCTURE   \\nMagnetic disks provide the bulk of secondary storage for modern computer systems.  \\nEach  disk  platter  has a flat circular  shape,  like a CD. Common  platter  diameters  range  from  \\n1.8 to 5.25  inches.  The  two  surfaces  of a platter  are covered  with  a magnetic  material.  We store  \\ninformation  by recording  it magnetically  on the platters.  \\nA read -write  head  \\\"flies\\\"  just above  each  surface  of every  platter.  The heads  are \\nattached  to a disk  arm,  which  moves  all the heads  as a unit.  The surface  of a platter  is \\nlogically  divided  into  circular  tracks,  which  are subdivided  into  sectors.  The set of tracks  that  \\nare at one arm position forms a cylinder. There may be thousands of concentric cylinders in a  \\ndisk  drive,  and each track  may  contain hundreds of sectors.  The storage  capacity  of common  \\ndisk  drives  is measured  in gigabytes.\", \"type\": \"Document\"}, {\"id\": null, \"metadata\": {\"source\": \"operating system.pdf\", \"page\": 62}, \"page_content\": \"Page  64                       \\n \\n \\n(Example  of segmentation)  \\n \\n \\nVIRTUAL  MEMORY  \\nVirtual memory is a technique that allows the execution of processes that may not be  \\ncompletely in memory. Only part of the program needs to be in memory for execution. It  \\nmeans that Logical address space can be much larger than physical address spa ce. Virtual  \\nmemory allows processes to easily share files and address spaces, and it provides an efficient  \\nmechanism  for process  creation.  \\nVirtual memory is the separation of user logical memory from physical memory. This  \\nseparation allows an extremely lar ge virtual memory to be provided for programmers when  \\nonly a smaller physical memory is available. Virtual memory makes the task of programming  \\nmuch  easier,  because  the programmer  no longer  needs  to worry  about  the amount  of physical  \\nmemory  available.\", \"type\": \"Document\"}]}"}